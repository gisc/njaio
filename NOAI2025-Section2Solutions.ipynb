{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ce3aac-82b3-4ade-8e40-d301e9e43cad",
   "metadata": {},
   "source": [
    "# Section 2: Programming Questions\n",
    "- This section contains 3 questions.\n",
    "- You are provided with the NumPy Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ad75ad-048c-45f0-9032-e6f893a6aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589b9dd-31e2-4b91-980d-cb4edaee9799",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 1: Linear Regression Using Gradient Descent\n",
    "***\n",
    "Linear Regression can also be performed using a technique called gradient descent, where the coefficients (or weights) of the model are iteratively adjusted to minimise a cost function (usually mean squared error). This method is particularly useful when the numbers of features is too large for analytical solutions like the normal equation or when the feature matrix is not invertible. <br><br>\n",
    "\n",
    "The gradient descent algorithm updates the weights by moving in the direction of the negative gradient of the cost function with respect to the weights. The updates occur iteratively until the algorithm converges to a minimum of the cost function. <br><br>\n",
    "\n",
    "The update rule for each weight is given by: <br>\n",
    "\n",
    "$$θ_j := θ_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_θ(x^i)-y^i)x_j^i$$\n",
    "\n",
    "**Explanation of Terms**\n",
    "1. $α$ is the learning rate.\n",
    "2. $m$ is the number of training examples.\n",
    "3. $h_θ(x^i)$ is the hypothesis function at iteration $i$.\n",
    "4. $x^i$ is the feature vector of $i_{th}$ training example.\n",
    "5. $y^i$ is the actual target value for the $i_{th}$ training example.\n",
    "6. $x_j^i$ is the value of feature $j$ for the $i_{th}$ training example.<br><br>\n",
    "\n",
    "**Key Points**\n",
    "- **Learning Rate**: The choice of learning rate is crucial for the convergence and performance of gradient descent.\n",
    "  - A small learning rate may lead to slow convergence.\n",
    "  - A large learning rate may cause overshooting and divergence.\n",
    "- **Number of Iterations**: The number of iterations determines how long the algorithm runs before it converges or stops.<br><br>\n",
    "\n",
    "**Practical Implementation**<br>\n",
    "Implementing gradient descent involves initialising the weights, computing the gradient of the cost function, and iteratively updating the weights according to the update rule.\n",
    "\n",
    "## Task #1\n",
    "- The function should take\n",
    "  - NumPy arrays X;\n",
    "  - y (target) as input;\n",
    "  - learning rate alpha;\n",
    "  - the number of iterations;\n",
    "  - return the coefficients of the linear regression model as a NumPy array.\n",
    "- Round your answer to four decimal places. -0.0 is a valid result for rounding a very small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dcc00b-7aee-46bf-88a0-a8dbfdbc0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 2], [2, 4], [2, 6]])\n",
    "y = np.array([2, 4, 6])\n",
    "alpha = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280aedd8-424d-452f-b102-45434de1706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y.reshape(-1, 1)\n",
    "        updates = X.T @ errors / m\n",
    "        theta -= alpha * updates\n",
    "    return np.round(theta.flatten(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185069c1-a46c-458d-911a-e224249c81ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.003 , 0.9987])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_gradient_descent(X=X, y=y, alpha=alpha, iterations=iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa58e3-05d0-401b-8561-b953abefa3d5",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2: K-Means Clustering\n",
    "***\n",
    "1. **Initialisation**<br>\n",
    "  Use the provided `initial_centroids` as your starting point.<br>\n",
    "\n",
    "2. **Assignment Step**<br>\n",
    "  For each point in your dataset:\n",
    "    - Calculate its distance to each centroid.\n",
    "    - Assign the point to the cluster of the nearest centroid.<br>\n",
    "      *Hint: Consider creating a helper function to calculate the Euclidean distance between two points.*\n",
    "\n",
    "3. **Update Step**<br>\n",
    "  For each cluster:\n",
    "    - Calculate the mean of all points assigned to the cluster.\n",
    "    - Update the centroid to this new mean position.<br>\n",
    "      *Hint: Be careful with potential empty clusters. Decide how you will handle them (eg. keep the previous centroid).*\n",
    "\n",
    "4. **Iteration**<br>\n",
    "  Repeat Step 2 and 3 until either:\n",
    "    - The centroids no longer change significantly, OR\n",
    "    - `max_iterations` reached.<br>\n",
    "      *Hint: You might want to keep track of the previous centroids to check for significant changes.*\n",
    "\n",
    "5. **Result**<br>\n",
    "  Return the list of final centroids, ensuring each coordinate is rounded to the nearest fourth decimal.\n",
    "\n",
    "## Task #2\n",
    "- Write a Python function that implements the K-Means clustering algorithm.\n",
    "  - This function should take specific inputs and produce a list of final centroids.\n",
    "  - K-Means clustering is a method used to partition *n* points into *k* clusters.\n",
    "  - The goal is to group similar points together and represent each group by its centroid.\n",
    "\n",
    "- Function Inputs:\n",
    "  - points: list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n",
    "  - *k*: an integer representing the number of clusters to form\n",
    "  - `initial_centroids`: list of initial centroid points, each a tuple of coordinates\n",
    "  - `max_iterations`: an integer representing the maximum number of iterations to perform\n",
    "\n",
    "- Function Output:\n",
    "  - A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0ace16-d8b0-46e1-8d7f-11b1da66da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(2, 4), (2, 8), (2, 10), (10, 2), (10, 4), (10, 0)]\n",
    "k = 2\n",
    "initial_centroids = [(2, 2), (10, 1)]\n",
    "max_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4efa2b0-b2a9-4d20-ba16-9a68eb002334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(((a - b) ** 2).sum(axis=1))\n",
    "\n",
    "def k_means_clustering(points, k, initial_centroids, max_iterations):\n",
    "    points = np.array(points)\n",
    "    centroids = np.array(initial_centroids)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        # Assign points to the nearest centroid\n",
    "        distances = np.array([euclidean_distance(points, centroid) for centroid in centroids])\n",
    "        assignments = np.argmin(distances, axis=0)\n",
    "\n",
    "        new_centroids = np.array([points[assignments == i].mean(axis=0) if len(points[assignments == i]) > 0 else centroids[i] for i in range(k)])\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        centroids = np.round(centroids,4)\n",
    "    return [tuple(centroid) for centroid in centroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315c4048-725c-4980-8e54-17d2d1882c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0, 7.3333), (10.0, 2.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means_clustering(points=points, k=k, initial_centroids=initial_centroids, max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190795a-4ac3-4610-b3e8-b84b6a5e7b91",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 3: Retrieval Augmented Generation\n",
    "***\n",
    "### Background\n",
    "\n",
    "In a classroom filled with curious students, Ms. Sally stood at the front with a confident smile.\n",
    "\n",
    "She had promised her class an engaging lesson on a cutting-edge technology: Artificial Intelligence.\n",
    "\n",
    "\"Alright, class,\" she began, holding a few printed notes in her hand. \"Let's build a smart assistant that can answer your questions using only a set of documents. \n",
    "\n",
    "Let’s say the question is: ‘How does machine learning work?’\n",
    "\n",
    "The smart assistant is to extract the content from the given set of documents and answer the question.\n",
    "\n",
    "This is done with a 2-step process: \"Retrieve and Generate\"\n",
    "\n",
    "The set of documents can be found below:\n",
    "\n",
    "1. Artificial intelligence is transforming industries.\n",
    "2. Machine learning is a subset of artificial intelligence.\n",
    "3. Deep learning allows machines to solve complex problems.\n",
    "4. Data science involves statistics and machine learning.\n",
    "5. Neural networks are inspired by the human brain.\n",
    "\n",
    "\"Which one of these do you think would help answer the question?\" she asked. \n",
    "\n",
    "A few hands shot up and answered it.\n",
    "\n",
    "\"That’s right! The second and fourth ones mentioned machine learning directly,\" she confirmed.\n",
    "\n",
    "\"Now the smart assistant has to take what it found and turn it into a formulated response: ‘Machine learning is a subset of artificial intelligence that enables systems to learn from data. It involves techniques like statistics and algorithms often used in data science.’ \n",
    "\n",
    "See how it combines ideas from the documents to explain the concept?\"\n",
    "\n",
    "## Task #3\n",
    "\n",
    "The goal is to retrieve the most relevant information from these documents and generate an accurate, concise, and meaningful response to the query.\n",
    "\n",
    "- The solution will be a simple Retrieval Augumented Generation solution consisting of the following:\n",
    "  - Feature Extraction such as vectorisation technique to represent the documents as numerical vectors that capture their semantic meaning and relevance to the question.\n",
    "  - Similarity Scoring to compute the relevance of each document to the query.\n",
    "\n",
    "### Expected output\n",
    "\n",
    "Query: How does machine learning work?\n",
    "\n",
    "Generated Response:\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn from data. It involves techniques like statistics and algorithms, often used in data science.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b81d61-5e7f-4e5c-b5ab-97156ce6d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of documents\n",
    "documents = [\n",
    "    \"Artificial intelligence is transforming industries.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning allows machines to solve complex problems.\",\n",
    "    \"Data science involves statistics and machine learning.\",\n",
    "    \"Neural networks are inspired by the human brain.\"\n",
    "]\n",
    "\n",
    "# Query\n",
    "query = \"How does machine learning work?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd5e010b-6df3-4cd6-a0bf-26ccdd48ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(\" \".join(documents + [query]).lower().split()))\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bb31dec-05e6-43d7-b535-6c5fa62b0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute TF vector\n",
    "def compute_tf(text):\n",
    "    tf_vector = np.zeros(len(unique_words))\n",
    "    words = text.lower().split()\n",
    "    for word in words:\n",
    "        if word in word_to_index:\n",
    "            tf_vector[word_to_index[word]] += 1\n",
    "    return tf_vector / (len(words) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3376d842-3521-4d1a-92a4-609bc6e55456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2.T)\n",
    "    norm_vec1 = np.linalg.norm(vec1, axis=1, keepdims=True)\n",
    "    norm_vec2 = np.linalg.norm(vec2, axis=1, keepdims=True)\n",
    "    return dot_product / (norm_vec1 * norm_vec2.T + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39fe143-b093-4774-bb01-98e2249dd0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infer response\n",
    "def infer_response(docs):\n",
    "    keywords = set()\n",
    "    for doc in docs:\n",
    "        keywords.update(doc.lower().split())\n",
    "    \n",
    "    if \"machine\" in keywords and \"science\" in keywords:\n",
    "        return (\"Machine learning is a subset of artificial intelligence that enables systems to learn from data. It involves techniques like statistics and algorithms, often used in data science.\")\n",
    "    else:\n",
    "        return \"I don't know.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89fbec3-fb42-4b28-87ea-1b4f8cad7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute document TF vectors\n",
    "doc_embeddings = np.array([compute_tf(doc) for doc in documents])\n",
    "query_embedding = compute_tf(query).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d1109a9-4618-4e76-86bd-9861cfafad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(doc_embeddings, query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce555a37-cc46-4a87-87d1-af7098259483",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices = similarities.flatten().argsort()[-2:][::-1]\n",
    "top_documents = [documents[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2cf338c-9ae6-4f92-b0f5-ea0e6dc43176",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = infer_response(top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c215d06-6a9e-4ee5-8ad2-84b9af2d8c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does machine learning work?\n",
      "\n",
      "Retrieved Documents:\n",
      "- Machine learning is a subset of artificial intelligence.\n",
      "- Data science involves statistics and machine learning.\n",
      "\n",
      "Generated Response:\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn from data. It involves techniques like statistics and algorithms, often used in data science.\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:\", query)\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for doc in top_documents:\n",
    "    print(\"-\", doc)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

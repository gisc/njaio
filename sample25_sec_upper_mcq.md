# NJAIO 2026 Sample Paper Section A MCQ
## Upper Secondary (Grades 9 - 12)

## Question 1
**What does the term "digital divide" mean in the context of AI?**

A) The gap between those who have access to AI technology and education and those who do not, often along socioeconomic or geographical lines  
B) The computational divide between classical binary systems and quantum computing architectures used in AI development  
C) The mathematical division algorithm that AI systems use to process numerical data in neural networks  
D) The professional separation between academic AI researchers and industry practitioners working on commercial applications

**Answer: A**

**Explanation:** The "AI digital divide" refers to inequality in AI benefits and knowledge. Some people, communities, or countries have the resources, education, and infrastructure to leverage AI, while others are left behind. This gap can lead to further economic and social inequality. It's not about how computers compute (digital vs analog), but about who gets to use and benefit from cutting-edge tech.

---

## Question 2
**AI is expected to automate certain jobs. What is a constructive strategy to handle job displacement caused by AI?**

A) Invest in retraining and upskilling programs so that workers can transition to new roles that AI creates or roles that require human skills  
B) Implement universal AI bans across all industries to preserve traditional employment structures and prevent technological disruption  
C) Adopt a laissez-faire approach where market forces naturally redistribute displaced workers without intervention  
D) Restrict AI development to only those companies that guarantee no worker displacement through legally binding agreements

**Answer: A**

**Explanation:** The realistic and positive approach is to help the workforce adapt. History shows technology creates new jobs even as it replaces some – the key is education and reskilling.

---

## Question 3
**In an autonomous car "trolley problem" scenario, what does this illustrate?**

A) An ethical dilemma in AI decision-making, where the AI must be programmed to handle situations with no good outcome  
B) A straightforward optimization problem that can be solved with sufficient sensor data and computational power  
C) A purely theoretical thought experiment that has no practical relevance to real-world autonomous vehicle deployment  
D) A minor navigation system malfunction rather than a fundamental moral programming challenge

**Answer: A**

**Explanation:** The trolley problem forces programmers and society to consider how an AI should be designed to value lives and make moral choices.

---

## Question 4
**What is one approach governments are considering to ensure AI is developed and used responsibly?**

A) Creating regulations and guidelines that require AI systems to meet certain standards for fairness, transparency, and safety  
B) Eliminating all regulatory oversight to allow unrestricted AI innovation and market-driven development  
C) Enforcing complete moratoriums on AI research and development until perfect safety can be guaranteed  
D) Delegating regulatory authority to AI systems themselves, allowing algorithms to self-govern without human intervention

**Answer: A**

**Explanation:** Governance includes laws or guidelines to prevent harmful bias, protect privacy, require transparency, and ensure accountability.

---

## Question 5
**"Black box" AI refers to a system whose inner workings are not easily understood. Why is a black-box AI often seen as problematic?**

A) If we can't understand how it makes decisions, it's hard to trust, debug, or ethically manage its outcomes  
B) The physical hardware components are literally housed in dark-colored enclosures that conceal their operations  
C) Opaque algorithms inherently use outdated computational methods that reduce system performance  
D) Transparent AI systems inherently process data more slowly than their opaque counterparts

**Answer: A**

**Explanation:** In high-stakes areas, not knowing why the AI decided something can be dangerous or unacceptable.

---

## Question 6
**If an AI system causes harm (e.g., an autonomous drone injures someone), who is accountable?**

A) The humans and organizations that designed, trained, or deployed the AI  
B) The AI system should be granted legal personhood and held accountable through algorithmic liability frameworks  
C) No entity bears responsibility when AI systems cause harm  
D) Accountability should be limited to shutting down the offending system, with no further consequences

**Answer: A**

**Explanation:** Accountability traces back to developers, manufacturers, and operators. AI has no legal personhood.

---

## Question 7
**In medical diagnostics, which performance metric is crucial when we want to catch every possible case?**

A) Recall – find as many true cases as possible, minimizing false negatives  
B) Precision – ensure every positive prediction is correct, even if some cases are missed  
C) Latency – ensure fast UI response  
D) Computational efficiency – minimize computational resources

**Answer: A**

**Explanation:** High recall is paramount because missing a sick patient can be life-threatening.

---

## Question 8
**Which of the following is an example of a machine learning model used for classification tasks?**

A) A decision tree that splits data based on features to decide a discrete class  
B) A linear regression model that predicts students' exam scores based on input features  
C) A deterministic sorting algorithm that alphabetically orders text strings  
D) A validation function that randomizes data splits

**Answer: A**

**Explanation:** Decision trees learn decision splits to classify data; the other options are not ML classification models.

---

## Question 9
**What is overfitting in the context of machine learning?**

A) Excellent training performance but poor generalization to unseen data  
B) A model too simple that performs poorly on both training and test data  
C) Training continues excessively, causing random hallucinations  
D) Memory limitations due to dataset size exceeding RAM

**Answer: A**

**Explanation:** Overfitting is learning noise/quirks of training data, harming performance on new data.

---

## Question 10
**In a neural network, what is adjusted during training for the model to learn?**

A) The weights between neurons  
B) The physical semiconductor architecture of the hardware  
C) The number of hidden layers  
D) The number of connections between neurons

**Answer: A**

**Explanation:** Training updates weights (and biases) via algorithms like backpropagation.

---

## Question 11
**Which scenario is an example of unsupervised learning?**

A) Grouping unlabeled data points into clusters based on similarity  
B) A classifier trained on labeled images of cats vs dogs  
C) An agent learning via rewards in an environment  
D) Rule-based document classification

**Answer: A**

**Explanation:** Unsupervised learning finds structure in unlabeled data; clustering is textbook.

---

## Question 12
**Which of these tasks is a regression problem?**

A) Predicting the temperature for tomorrow in degrees Celsius  
B) Determining whether an email is "spam" or "not spam"  
C) Recognizing if an image contains a cat or a dog  
D) Classifying handwritten digits (0–9) from images

**Answer: A**

**Explanation:** Regression predicts continuous values; the others are classification tasks.

---

## Question 13
**Which is an example of AI for social good aligned with the UN’s SDGs?**

A) Monitoring crop health via drone images to help farmers increase food production  
B) An AI design tool that enhances presentation slides  
C) An AI application that limits user screen time and digital addiction  
D) Publishing ethical guidelines for AI in assessments

**Answer: A**

**Explanation:** Monitoring crops addresses SDG2 and SDG12; others are less aligned or tangential.

---

## Question 14
**Predictive policing using historical crime data: what is a key fairness concern?**

A) Reinforcing existing policing biases, creating feedback loops targeting the same communities  
B) AI directly causes criminal activity via prophecies  
C) Criminals will inevitably hack predictions to misdirect resources  
D) No concern; historical data provides objective truth

**Answer: A**

**Explanation:** Historical enforcement patterns can encode bias leading to unfair targeting.

---

## Question 15
**Credit scoring AI gives lower scores to certain minority groups despite similar histories. What does this indicate?**

A) Discriminatory bias learned from data or proxies for sensitive attributes  
B) Genuine differences in financial behavior previously missed  
C) Algorithms cannot exhibit bias  
D) Pure random noise unrelated to group membership

**Answer: A**

**Explanation:** Systematic disparities indicate bias; features may proxy sensitive factors (e.g., zip code).

---

## Question 16
**How does AGI differ from today’s narrow AI?**

A) AGI has broad human-level intelligence across domains; narrow AI is specialized  
B) AGI requires quantum computing; narrow AI uses classical computing  
C) No difference; AGI is just marketing  
D) Narrow AI has emotions; AGI is purely logical

**Answer: A**

**Explanation:** AGI is hypothetical flexible general intelligence; today’s systems are narrow.

---

## Question 17
**In AI, what is an agent?**

A) A system that perceives its environment and acts to achieve goals  
B) A human intelligence operative using AI tools  
C) A specialized hardware component  
D) A simple IF-THEN rule with no feedback

**Answer: A**

**Explanation:** Agents sense and act; examples include robot vacuums or game bots.

---

## Question 18
**Which is NOT a core ethical principle for AI development and use?**

A) Performance and accuracy of AI models at reasonable cost  
B) Fairness and non-discrimination  
C) Transparency and explainability  
D) Accountability for decisions and impacts

**Answer: A**

**Explanation:** Ethical principles emphasize fairness, transparency, accountability, privacy, safety.

---

## Question 19
**Why incorporate regularization or cross-validation during training?**

A) To prevent overfitting and improve generalization to unseen data  
B) To accelerate training time  
C) To reduce model complexity purely for lower resource use  
D) To encrypt model parameters

**Answer: A**

**Explanation:** These methods combat overfitting to maintain generality beyond the training set.

---

## Question 20
**An AI company scrapes millions of personal social media posts to train a chatbot, without consent. Which principle is violated most directly?**

A) Privacy – personal data taken and used without permission  
B) Environmental sustainability – excessive energy  
C) Algorithmic accountability – model not responsible for sources  
D) Security – scraping may expose users to risks

**Answer: A**

**Explanation:** Taking personal data without consent breaches privacy and may raise IP issues.
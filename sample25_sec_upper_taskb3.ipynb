{
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":["# Task B3 — Minimal AI Agents (NumPy)\n","Upper Secondary / Section B — Structured Tasks\n\n","Constraints: Use Python 3 and NumPy only. No external LLM calls. We simulate planning with a deterministic stub.\n\n","Scope:\n","- Create a minimal LLM-enabled agent with tool use and step-wise planning (simulated).\n","- Compare single vs multi-agent designs (roles and coordination).\n","- Evaluate with success rate, error taxonomy, and simple cost/time counts.\n","- Apply guardrails: permissions, sandboxing, rate limits, and transparent logs."]},
  {"cell_type":"markdown","metadata":{},"source":["## Structured Theory Questions\n","1. Sketch the agent loop (sense → plan → act). What state is tracked?\n","2. List two risks of autonomous tool-use and one guardrail for each.\n","3. Give two evaluation metrics suitable for small deterministic tasks."]},
  {"cell_type":"markdown","metadata":{},"source":["## Design Challenge\n","Design two flows:\n","- Single-agent (generalist)\n","- Dual-agent (Planner + Executor)\n\n","Describe allowed tools, max steps, and the logging fields you will capture (e.g., tool name, inputs, outputs, error).\n"]},
  {"cell_type":"code","metadata":{},"execution_count":null,"source":["import numpy as np"]},
  {"cell_type":"markdown","metadata":{},"source":["## Provided Tasks (array-based sandbox)"]},
  {"cell_type":"code","metadata":{},"execution_count":null,"source":["# DO NOT MODIFY\n","TASKS = [\n","    {'name':'compute sum','type':'sum','input': np.array([1,2,3])},\n","    {'name':'compute mean','type':'mean','input': np.array([4,6,10])},\n","    {'name':'threshold classify','type':'threshold','input': np.array([0.2, 0.9, 0.4]), 'threshold': 0.5},\n","    {'name':'compute mean','type':'mean','input': np.array([10,0])}\n","]\n","ALLOWED_TOOLS = {'sum_array','mean_array','threshold_classify'}\n","MAX_STEPS = 3"]},
  {"cell_type":"markdown","metadata":{},"source":["## Tools (NumPy-only)"]},
  {"cell_type":"code","metadata":{},"execution_count":null,"source":["def sum_array(arr: np.ndarray):\n","    return float(np.sum(arr))\n","def mean_array(arr: np.ndarray):\n","    return float(np.mean(arr))\n","def threshold_classify(arr: np.ndarray, threshold: float=0.5):\n","    return (arr >= threshold).astype(int)\n","TOOLS = {\n","    'sum_array': sum_array,\n","    'mean_array': mean_array,\n","    'threshold_classify': threshold_classify\n","}"]},
  {"cell_type":"markdown","metadata":{},"source":["## Deterministic Planner (simulated LLM)\n","Maps a task dict to a tool call plan (list of steps)."]},
  {"cell_type":"code","metadata":{},"execution_count":null,"source":["def planner_stub(task):\n","    t = task.get('type')\n","    if t == 'sum':\n","        return [{'tool':'sum_array','args':{'arr': task['input']}}]\n","    if t == 'mean':\n","        return [{'tool':'mean_array','args':{'arr': task['input']}}]\n","    if t == 'threshold':\n","        return [{'tool':'threshold_classify','args':{'arr': task['input'], 'threshold': task.get('threshold',0.5)}}]\n","    return []  # unknown task"]},
  {"cell_type":"markdown","metadata":{},"source":["## Agent Implementations\n","- Guardrails: allowed tools, max steps, logging\n","- SingleAgent executes planner_stub internally\n","- PlannerAgent creates plan; ExecutorAgent executes it"]},
  {"cell_type":"code","metadata":{},"execution_count":null,"source":["class SingleAgent:\n","    def __init__(self, allowed_tools, max_steps):\n","        self.allowed_tools = allowed_tools\n","        self.max_steps = max_steps\n","        self.logs = []\n","    def run(self, task):\n","        plan = planner_stub(task)\n","        steps = 0\n","        last_out = None\n","        for step in plan:\n","            steps += 1\n","            if steps > self.max_steps:\n","                self.logs.append({'error':'max_steps_exceeded'})\n","                return None\n","            tool = step['tool']\n","            if tool not in self.allowed_tools:\n","                self.logs.append({'error':'tool_not_allowed', 'tool':tool})\n","                return None\n","            fn = TOOLS[tool]\n","            out = fn(**step['args'])\n","            self.logs.append({'tool':tool,'args':step['args'],'out':out})\n","            last_out = out\n","        return last_out\n","\n","class PlannerAgent:\n","    def plan(self, task):\n","        return planner_stub(task)\n","\n","class ExecutorAgent:\n","    def __init__(self, allowed_tools, max_steps):\n","        self.allowed_tools = allowed_tools\n","        self.max_steps = max_steps\n","        self.logs = []\n","    def execute(self, plan):\n","        steps = 0\n","        last_out = None\n","        for step in plan:\n","            steps += 1\n","            if steps > self.max_steps:\n","                self.logs.append({'error':'max_steps_exceeded'})\n","                return None\n","            tool = step['tool']\n","            if tool not in self.allowed_tools:\n","                self.logs.append({'error':'tool_not_allowed', 'tool':tool})\n","                return None\n","            fn = TOOLS[tool]\n","            out = fn(**step['args'])\n","            self.logs.append({'tool':tool,'args':step['args'],'out':out})\n","            last_out = out\n","        return last_out"]},
  {"cell_type":"markdown","metadata":{},"source":["## Evaluation Metrics\n","We evaluate success on the provided TASKS, track errors, and count steps."]},
  {"cell_type":"code","metadata":{},"execution_count":null,"source":["def evaluate_single(agent, tasks):\n","    successes, errors, steps = 0, {}, 0\n","    outputs = []\n","    for t in tasks:\n","        out = agent.run(t)\n","        steps += len(planner_stub(t))\n","        outputs.append(out)\n","        if out is None:\n","            for log in agent.logs[::-1]:\n","                if 'error' in log:\n","                    errors[log['error']] = errors.get(log['error'],0)+1\n","                    break\n","        else:\n","            successes += 1\n","    return {'successes':successes,'errors':errors,'steps':steps,'outputs':outputs}\n","\n","def evaluate_dual(planner, executor, tasks):\n","    successes, errors, steps = 0, {}, 0\n","    outputs = []\n","    for t in tasks:\n","        plan = planner.plan(t)\n","        steps += len(plan)\n","        out = executor.execute(plan)\n","        outputs.append(out)\n","        if out is None:\n","            for log in executor.logs[::-1]:\n","                if 'error' in log:\n","                    errors[log['error']] = errors.get(log['error'],0)+1\n","                    break\n","        else:\n","            successes += 1\n","    return {'successes':successes,'errors':errors,'steps':steps,'outputs':outputs}"]},
  {"cell_type":"code","metadata":{},"execution_count":null,"source":["# Self-check tests\n","single = SingleAgent(ALLOWED_TOOLS, MAX_STEPS)\n","res_single = evaluate_single(single, TASKS)\n","print('single:', res_single)\n","planner = PlannerAgent()\n","executor = ExecutorAgent(ALLOWED_TOOLS, MAX_STEPS)\n","res_dual = evaluate_dual(planner, executor, TASKS)\n","print('dual:', res_dual)\n","# Expect at least 3 successes out of 4 for both modes\n","assert res_single['successes'] >= 3\n","assert res_dual['successes'] >= 3\n","# Outputs sanity\n","assert isinstance(res_single['outputs'][0], float)\n","assert isinstance(res_dual['outputs'][1], float)\n","print('All tests passed.')"]},
  {"cell_type":"markdown","metadata":{},"source":["## Reflection\n","Explain the guardrails, logs, and trade-offs observed in the single vs multi-agent setup."]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
